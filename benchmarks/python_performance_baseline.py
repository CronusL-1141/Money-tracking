"""
Pythonæ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
ä¸ºRustè¿ç§»æä¾›å‡†ç¡®çš„æ€§èƒ½å¯¹æ¯”åŸºå‡†

åŠŸèƒ½:
1. æµ‹é‡FIFOå’Œå·®é¢è®¡ç®—æ³•çš„å¤„ç†æ—¶é—´
2. ç›‘æ§å†…å­˜ä½¿ç”¨é‡å˜åŒ–
3. ç”Ÿæˆè¾“å‡ºç»“æœå“ˆå¸Œå€¼ç”¨äºä¸€è‡´æ€§éªŒè¯
4. æ”¯æŒå¤šè½®æµ‹è¯•å’Œç»Ÿè®¡åˆ†æ
5. ç”Ÿæˆè¯¦ç»†çš„åŸºå‡†æŠ¥å‘Š
"""

import os
import sys
import time
import hashlib
import psutil
import json
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import statistics
import tracemalloc
import gc

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), 'src'))

try:
    from services.audit_service import AuditService
    from core.factories.tracker_factory import TrackerFactory
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

class PerformanceBenchmark:
    """Pythonæ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_data_dir = "rust-backend/test_data"
        self.output_dir = "benchmarks/results"
        self.algorithms = ["FIFO", "BALANCE_METHOD"]
        
        # æµ‹è¯•æ•°æ®é›†é…ç½®
        self.test_datasets = {
            "minimal": {
                "file": "test_data_minimal.xlsx",
                "description": "æœ€å°æ•°æ®é›†(50è¡Œ) - åŸºç¡€åŠŸèƒ½æµ‹è¯•",
                "expected_rows": 50
            },
            "standard": {
                "file": "test_data_standard.xlsx", 
                "description": "æ ‡å‡†æ•°æ®é›†(1000è¡Œ) - å¸¸è§„æ€§èƒ½åŸºå‡†",
                "expected_rows": 1000
            },
            "investment": {
                "file": "test_data_investment.xlsx",
                "description": "æŠ•èµ„äº§å“æ•°æ®é›† - å¤æ‚ä¸šåŠ¡é€»è¾‘æµ‹è¯•", 
                "expected_rows": 500
            },
            "complex": {
                "file": "test_data_complex.xlsx",
                "description": "å¤æ‚æ•°æ®é›†(10000è¡Œ) - å¤§æ•°æ®é‡æ€§èƒ½æµ‹è¯•",
                "expected_rows": 10000
            }
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
        print("ğŸš€ Pythonæ€§èƒ½åŸºå‡†æµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def run_full_benchmark(self, rounds: int = 3) -> Dict:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸ“Š å¼€å§‹å®Œæ•´åŸºå‡†æµ‹è¯• (å…±{rounds}è½®)")
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "system_info": self._get_system_info(),
            "test_rounds": rounds,
            "algorithms": {},
            "summary": {}
        }
        
        for algorithm in self.algorithms:
            print(f"\nğŸ”„ æµ‹è¯•ç®—æ³•: {algorithm}")
            results["algorithms"][algorithm] = self._benchmark_algorithm(algorithm, rounds)
        
        # ç”Ÿæˆæ€»ç»“
        results["summary"] = self._generate_summary(results["algorithms"])
        
        # ä¿å­˜ç»“æœ
        self._save_results(results)
        
        # æ˜¾ç¤ºæŠ¥å‘Š
        self._display_report(results)
        
        return results
    
    def _benchmark_algorithm(self, algorithm: str, rounds: int) -> Dict:
        """å¯¹å•ä¸ªç®—æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        algorithm_results = {
            "algorithm": algorithm,
            "datasets": {},
            "overall_stats": {}
        }
        
        for dataset_name, dataset_info in self.test_datasets.items():
            print(f"  ğŸ“ æµ‹è¯•æ•°æ®é›†: {dataset_name}")
            
            dataset_results = []
            file_path = os.path.join(self.test_data_dir, dataset_info["file"])
            
            if not os.path.exists(file_path):
                print(f"    âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            for round_num in range(rounds):
                print(f"    ğŸ”„ ç¬¬{round_num + 1}è½®æµ‹è¯•...")
                try:
                    result = self._run_single_test(algorithm, file_path, dataset_info)
                    result["round"] = round_num + 1
                    dataset_results.append(result)
                    
                    # æ˜¾ç¤ºå®æ—¶ç»“æœ
                    print(f"      â±ï¸ å¤„ç†æ—¶é—´: {result['processing_time']:.3f}s")
                    print(f"      ğŸ’¾ å³°å€¼å†…å­˜: {result['peak_memory_mb']:.1f}MB")
                    print(f"      ğŸ“Š å¤„ç†è¡Œæ•°: {result['processed_rows']:,}")
                    
                except Exception as e:
                    print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                    continue
            
            if dataset_results:
                algorithm_results["datasets"][dataset_name] = {
                    "info": dataset_info,
                    "results": dataset_results,
                    "stats": self._calculate_stats(dataset_results)
                }
        
        return algorithm_results
    
    def _run_single_test(self, algorithm: str, file_path: str, dataset_info: Dict) -> Dict:
        """è¿è¡Œå•æ¬¡æµ‹è¯•"""
        # æ¸…ç†å†…å­˜
        gc.collect()
        
        # å¼€å§‹å†…å­˜è¿½è¸ª
        tracemalloc.start()
        process = psutil.Process()
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            # æ‰§è¡Œç®—æ³•
            audit_service = AuditService(algorithm=algorithm)
            result_df = audit_service.analyze_financial_data(file_path, suppress_output=True)
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            processing_time = end_time - start_time
            
            # è®¡ç®—å†…å­˜ä½¿ç”¨
            current_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_usage = current_memory - start_memory
            
            # è·å–å†…å­˜è¿½è¸ªä¿¡æ¯
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            peak_memory_mb = peak / 1024 / 1024
            
            # ç”Ÿæˆç»“æœå“ˆå¸Œ
            result_hash = self._generate_result_hash(result_df, audit_service.tracker)
            
            # è·å–è¿½è¸ªå™¨çŠ¶æ€
            tracker_state = audit_service.tracker.è·å–çŠ¶æ€æ‘˜è¦() if hasattr(audit_service.tracker, 'è·å–çŠ¶æ€æ‘˜è¦') else {}
            
            return {
                "processing_time": processing_time,
                "memory_usage_mb": memory_usage,
                "peak_memory_mb": peak_memory_mb,
                "processed_rows": len(result_df) if result_df is not None else 0,
                "result_hash": result_hash,
                "tracker_state": {
                    "ä¸ªäººä½™é¢": getattr(audit_service.tracker, 'ä¸ªäººä½™é¢', 0),
                    "å…¬å¸ä½™é¢": getattr(audit_service.tracker, 'å…¬å¸ä½™é¢', 0),
                    "ç´¯è®¡æŒªç”¨é‡‘é¢": getattr(audit_service.tracker, 'ç´¯è®¡æŒªç”¨é‡‘é¢', 0),
                    "ç´¯è®¡å«ä»˜é‡‘é¢": getattr(audit_service.tracker, 'ç´¯è®¡å«ä»˜é‡‘é¢', 0)
                },
                "success": True
            }
            
        except Exception as e:
            tracemalloc.stop()
            return {
                "processing_time": 0,
                "memory_usage_mb": 0,
                "peak_memory_mb": 0,
                "processed_rows": 0,
                "result_hash": "",
                "tracker_state": {},
                "success": False,
                "error": str(e)
            }
    
    def _generate_result_hash(self, result_df: pd.DataFrame, tracker) -> str:
        """ç”Ÿæˆç»“æœå“ˆå¸Œå€¼ç”¨äºä¸€è‡´æ€§éªŒè¯"""
        if result_df is None:
            return ""
        
        # é€‰æ‹©å…³é”®å­—æ®µç”Ÿæˆå“ˆå¸Œ
        key_columns = ['ä¸ªäººèµ„é‡‘å æ¯”', 'å…¬å¸èµ„é‡‘å æ¯”', 'è¡Œä¸ºæ€§è´¨', 'ç´¯è®¡æŒªç”¨', 'ç´¯è®¡å«ä»˜', 'ä½™é¢']
        available_columns = [col for col in key_columns if col in result_df.columns]
        
        if not available_columns:
            return ""
        
        # æå–å…³é”®æ•°æ®
        hash_data = []
        
        # DataFrameå…³é”®å­—æ®µ
        for col in available_columns:
            if result_df[col].dtype in ['float64', 'int64']:
                # æ•°å€¼å­—æ®µä¿ç•™2ä½å°æ•°
                hash_data.extend([f"{x:.2f}" for x in result_df[col].fillna(0)])
            else:
                # å­—ç¬¦ä¸²å­—æ®µ
                hash_data.extend([str(x) for x in result_df[col].fillna("")])
        
        # è¿½è¸ªå™¨çŠ¶æ€
        if hasattr(tracker, 'ä¸ªäººä½™é¢'):
            hash_data.append(f"{tracker.ä¸ªäººä½™é¢:.2f}")
        if hasattr(tracker, 'å…¬å¸ä½™é¢'):
            hash_data.append(f"{tracker.å…¬å¸ä½™é¢:.2f}")
        if hasattr(tracker, 'ç´¯è®¡æŒªç”¨é‡‘é¢'):
            hash_data.append(f"{tracker.ç´¯è®¡æŒªç”¨é‡‘é¢:.2f}")
        if hasattr(tracker, 'ç´¯è®¡å«ä»˜é‡‘é¢'):
            hash_data.append(f"{tracker.ç´¯è®¡å«ä»˜é‡‘é¢:.2f}")
        
        # ç”ŸæˆMD5å“ˆå¸Œ
        hash_string = "|".join(hash_data)
        return hashlib.md5(hash_string.encode('utf-8')).hexdigest()[:16]  # å–å‰16ä½
    
    def _calculate_stats(self, results: List[Dict]) -> Dict:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        if not results or not any(r.get("success", False) for r in results):
            return {"error": "æ— æœ‰æ•ˆç»“æœ"}
        
        valid_results = [r for r in results if r.get("success", False)]
        
        times = [r["processing_time"] for r in valid_results]
        memories = [r["peak_memory_mb"] for r in valid_results]
        
        return {
            "count": len(valid_results),
            "processing_time": {
                "mean": statistics.mean(times),
                "median": statistics.median(times),
                "min": min(times),
                "max": max(times),
                "stdev": statistics.stdev(times) if len(times) > 1 else 0
            },
            "memory_usage": {
                "mean": statistics.mean(memories),
                "median": statistics.median(memories),
                "min": min(memories),
                "max": max(memories),
                "stdev": statistics.stdev(memories) if len(memories) > 1 else 0
            },
            "result_hashes": list(set(r["result_hash"] for r in valid_results)),
            "consistency": len(set(r["result_hash"] for r in valid_results)) == 1
        }
    
    def _generate_summary(self, algorithm_results: Dict) -> Dict:
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
        summary = {
            "algorithms_tested": len(algorithm_results),
            "algorithm_comparison": {}
        }
        
        # ç®—æ³•å¯¹æ¯”
        if "FIFO" in algorithm_results and "BALANCE_METHOD" in algorithm_results:
            for dataset in self.test_datasets.keys():
                if (dataset in algorithm_results["FIFO"].get("datasets", {}) and 
                    dataset in algorithm_results["BALANCE_METHOD"].get("datasets", {})):
                    
                    fifo_stats = algorithm_results["FIFO"]["datasets"][dataset]["stats"]
                    balance_stats = algorithm_results["BALANCE_METHOD"]["datasets"][dataset]["stats"]
                    
                    if "error" not in fifo_stats and "error" not in balance_stats:
                        fifo_time = fifo_stats["processing_time"]["mean"]
                        balance_time = balance_stats["processing_time"]["mean"]
                        
                        summary["algorithm_comparison"][dataset] = {
                            "fifo_time_avg": fifo_time,
                            "balance_time_avg": balance_time,
                            "speed_ratio": balance_time / fifo_time if fifo_time > 0 else 0,
                            "faster_algorithm": "FIFO" if fifo_time < balance_time else "BALANCE_METHOD"
                        }
        
        return summary
    
    def _get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            "python_version": sys.version,
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "platform": sys.platform,
            "timestamp": datetime.now().isoformat()
        }
    
    def _save_results(self, results: Dict):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"python_baseline_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # åŒæ—¶ä¿å­˜ä¸€ä¸ªlatestç‰ˆæœ¬
        latest_filepath = os.path.join(self.output_dir, "python_baseline_latest.json")
        with open(latest_filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜:")
        print(f"  ğŸ“„ è¯¦ç»†ç»“æœ: {filepath}")
        print(f"  ğŸ”— æœ€æ–°ç»“æœ: {latest_filepath}")
    
    def _display_report(self, results: Dict):
        """æ˜¾ç¤ºæµ‹è¯•æŠ¥å‘Š"""
        print(f"\n" + "="*80)
        print(f"ğŸ“Š PYTHONæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print(f"="*80)
        print(f"ğŸ• æµ‹è¯•æ—¶é—´: {results['timestamp']}")
        print(f"ğŸ”„ æµ‹è¯•è½®æ•°: {results['test_rounds']}")
        print(f"ğŸ’» ç³»ç»Ÿä¿¡æ¯: Python {results['system_info']['python_version'].split()[0]}, {results['system_info']['cpu_count']}æ ¸CPU")
        
        for algorithm, algo_results in results["algorithms"].items():
            print(f"\nğŸ§® ç®—æ³•: {algorithm}")
            print(f"-" * 60)
            
            for dataset_name, dataset_results in algo_results.get("datasets", {}).items():
                stats = dataset_results["stats"]
                if "error" in stats:
                    print(f"  âŒ {dataset_name}: {stats['error']}")
                    continue
                
                info = dataset_results["info"]
                print(f"  ğŸ“ {dataset_name} ({info['description']})")
                print(f"    â±ï¸  å¹³å‡æ—¶é—´: {stats['processing_time']['mean']:.3f}s (Â±{stats['processing_time']['stdev']:.3f}s)")
                print(f"    ğŸ’¾ å¹³å‡å†…å­˜: {stats['memory_usage']['mean']:.1f}MB (Â±{stats['memory_usage']['stdev']:.1f}MB)")
                print(f"    ğŸƒâ€â™‚ï¸ å¤„ç†é€Ÿåº¦: {info['expected_rows']/stats['processing_time']['mean']:,.0f} è¡Œ/ç§’")
                print(f"    ğŸ”’ ç»“æœä¸€è‡´æ€§: {'âœ…' if stats['consistency'] else 'âŒ'}")
        
        # ç®—æ³•å¯¹æ¯”
        if "algorithm_comparison" in results["summary"] and results["summary"]["algorithm_comparison"]:
            print(f"\nâš–ï¸ ç®—æ³•æ€§èƒ½å¯¹æ¯”")
            print(f"-" * 60)
            for dataset, comparison in results["summary"]["algorithm_comparison"].items():
                print(f"  ğŸ“Š {dataset}:")
                print(f"    FIFO: {comparison['fifo_time_avg']:.3f}s")
                print(f"    å·®é¢æ³•: {comparison['balance_time_avg']:.3f}s") 
                print(f"    æ›´å¿«ç®—æ³•: {comparison['faster_algorithm']} (æ¯”ç‡: {comparison['speed_ratio']:.2f}x)")
        
        print(f"\nğŸ¯ åŸºå‡†æ•°æ®å·²å»ºç«‹ï¼Œå¯ç”¨äºRustç‰ˆæœ¬æ€§èƒ½å¯¹æ¯”ï¼")

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸš€ Pythonæ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·")
    print("ä¸ºRustè¿ç§»å»ºç«‹å‡†ç¡®çš„æ€§èƒ½å¯¹æ¯”åŸºå‡†")
    
    # æ£€æŸ¥æµ‹è¯•æ•°æ®
    test_data_dir = "rust-backend/test_data"
    if not os.path.exists(test_data_dir):
        print(f"âŒ æµ‹è¯•æ•°æ®ç›®å½•ä¸å­˜åœ¨: {test_data_dir}")
        print("è¯·å…ˆè¿è¡Œ analyze_data_format.py åˆ›å»ºæµ‹è¯•æ•°æ®é›†")
        return
    
    benchmark = PerformanceBenchmark()
    
    try:
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        results = benchmark.run_full_benchmark(rounds=3)
        
        print(f"\nâœ… PythonåŸºå‡†æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“ˆ åç»­å¯ä½¿ç”¨è¿™äº›åŸºå‡†æ•°æ®éªŒè¯Rustç‰ˆæœ¬æ€§èƒ½æå‡")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
