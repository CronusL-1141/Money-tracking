"""
åˆ†æç°æœ‰æ•°æ®æ ¼å¼å¹¶åˆ›å»ºæ ‡å‡†åŒ–æµ‹è¯•æ•°æ®é›†
"""
import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta
import random

def analyze_existing_data():
    """åˆ†æç°æœ‰æ•°æ®æ ¼å¼"""
    try:
        # è¯»å–ç°æœ‰æ•°æ®
        df = pd.read_excel('data/input/æµæ°´.xlsx')
        
        print('ğŸ” ç°æœ‰æ•°æ®åˆ†æ:')
        print(f'æ€»è¡Œæ•°: {len(df):,}')
        print(f'åˆ—æ•°: {len(df.columns)}')
        
        print('\nğŸ“‹ åˆ—åå’Œæ•°æ®ç±»å‹:')
        for i, col in enumerate(df.columns):
            non_null = df[col].notna().sum()
            print(f'{i+1:2d}. {col:<20} ({str(df[col].dtype):<10}) - {non_null:,}/{len(df):,} éç©º')
        
        print('\nğŸ“Š å‰5è¡Œæ•°æ®æ ·ä¾‹:')
        display_cols = df.columns[:6] if len(df.columns) > 6 else df.columns
        print(df[display_cols].head().to_string())
        
        if 'èµ„é‡‘å±æ€§' in df.columns:
            print('\nğŸ’° èµ„é‡‘å±æ€§ç±»å‹ç»Ÿè®¡:')
            fund_attrs = df['èµ„é‡‘å±æ€§'].value_counts()
            for attr, count in fund_attrs.head(15).items():
                print(f'  {attr}: {count:,}')
        
        if 'äº¤æ˜“æ—¥æœŸ' in df.columns:
            print('\nğŸ“… æ—¥æœŸèŒƒå›´åˆ†æ:')
            df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ'])
            print(f'æœ€æ—©æ—¥æœŸ: {df["äº¤æ˜“æ—¥æœŸ"].min()}')
            print(f'æœ€æ™šæ—¥æœŸ: {df["äº¤æ˜“æ—¥æœŸ"].max()}')
            print(f'æ—¥æœŸè·¨åº¦: {(df["äº¤æ˜“æ—¥æœŸ"].max() - df["äº¤æ˜“æ—¥æœŸ"].min()).days} å¤©')
        
        # åˆ†ææ•°å€¼å­—æ®µ
        numeric_cols = ['äº¤æ˜“æ”¶å…¥é‡‘é¢', 'äº¤æ˜“æ”¯å‡ºé‡‘é¢', 'ä½™é¢']
        for col in numeric_cols:
            if col in df.columns:
                print(f'\nğŸ“ˆ {col} ç»Ÿè®¡:')
                print(f'  æœ€å°å€¼: {df[col].min():,.2f}')
                print(f'  æœ€å¤§å€¼: {df[col].max():,.2f}')
                print(f'  å¹³å‡å€¼: {df[col].mean():,.2f}')
                print(f'  éé›¶ä¸ªæ•°: {(df[col] > 0).sum():,}')
        
        return df
        
    except Exception as e:
        print(f'âŒ è¯»å–åŸå§‹æ•°æ®å¤±è´¥: {e}')
        return None

def create_test_datasets(original_df):
    """åŸºäºåŸå§‹æ•°æ®åˆ›å»º5å¥—æµ‹è¯•æ•°æ®é›†"""
    if original_df is None:
        print('âŒ æ— æ³•åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ŒåŸå§‹æ•°æ®è¯»å–å¤±è´¥')
        return
    
    # ç¡®ä¿æµ‹è¯•æ•°æ®ç›®å½•å­˜åœ¨
    test_dir = 'rust-backend/test_data'
    os.makedirs(test_dir, exist_ok=True)
    
    print(f'\nğŸ—ï¸ å¼€å§‹åˆ›å»ºæµ‹è¯•æ•°æ®é›†åˆ°ç›®å½•: {test_dir}')
    
    # 1. Minimal Dataset (50è¡Œ) - åŸºç¡€åŠŸèƒ½æµ‹è¯•
    print('ğŸ“Š åˆ›å»º minimal æµ‹è¯•æ•°æ®é›† (50è¡Œ)...')
    minimal_df = original_df.head(50).copy()
    minimal_df.to_excel(f'{test_dir}/test_data_minimal.xlsx', index=False)
    
    # 2. Standard Dataset (1000è¡Œ) - å¸¸è§„åœºæ™¯æµ‹è¯•  
    print('ğŸ“Š åˆ›å»º standard æµ‹è¯•æ•°æ®é›† (1000è¡Œ)...')
    if len(original_df) >= 1000:
        standard_df = original_df.head(1000).copy()
    else:
        # å¦‚æœåŸå§‹æ•°æ®ä¸å¤Ÿ1000è¡Œï¼Œé‡å¤é‡‡æ ·
        standard_df = original_df.sample(n=min(1000, len(original_df)), replace=True).copy()
        standard_df = standard_df.reset_index(drop=True)
    standard_df.to_excel(f'{test_dir}/test_data_standard.xlsx', index=False)
    
    # 3. Investment Dataset - ä¸“é—¨åŒ…å«æŠ•èµ„äº§å“çš„æµ‹è¯•æ•°æ®
    print('ğŸ“Š åˆ›å»º investment æµ‹è¯•æ•°æ®é›†...')
    investment_keywords = ['ç†è´¢', 'æŠ•èµ„', 'ä¿é™©', 'å…³è”é“¶è¡Œå¡', 'èµ„é‡‘æ± ']
    if 'èµ„é‡‘å±æ€§' in original_df.columns:
        investment_mask = original_df['èµ„é‡‘å±æ€§'].str.contains('|'.join(investment_keywords), na=False)
        investment_rows = original_df[investment_mask]
        if len(investment_rows) > 0:
            # åŒ…å«æŠ•èµ„äº§å“çš„è®°å½• + ä¸€äº›æ™®é€šäº¤æ˜“è®°å½•
            normal_rows = original_df[~investment_mask].head(200)
            investment_df = pd.concat([investment_rows.head(300), normal_rows]).reset_index(drop=True)
        else:
            # å¦‚æœæ²¡æœ‰æŠ•èµ„äº§å“è®°å½•ï¼Œåˆ›å»ºä¸€äº›æ¨¡æ‹Ÿçš„
            investment_df = create_mock_investment_data(original_df)
    else:
        investment_df = original_df.head(500).copy()
    
    investment_df.to_excel(f'{test_dir}/test_data_investment.xlsx', index=False)
    
    # 4. Complex Dataset (10000è¡Œ) - å¤§æ•°æ®é‡æµ‹è¯•
    print('ğŸ“Š åˆ›å»º complex æµ‹è¯•æ•°æ®é›† (10000è¡Œ)...')
    if len(original_df) >= 10000:
        complex_df = original_df.head(10000).copy()
    else:
        # é‡å¤é‡‡æ ·åˆ›å»ºå¤§æ•°æ®é›†
        n_repeats = (10000 // len(original_df)) + 1
        complex_df = pd.concat([original_df] * n_repeats).head(10000).reset_index(drop=True)
    complex_df.to_excel(f'{test_dir}/test_data_complex.xlsx', index=False)
    
    # 5. Integrity Issues Dataset - åŒ…å«å®Œæ•´æ€§é—®é¢˜çš„æµ‹è¯•æ•°æ®
    print('ğŸ“Š åˆ›å»º integrity_issues æµ‹è¯•æ•°æ®é›†...')
    integrity_df = create_integrity_issues_data(original_df)
    integrity_df.to_excel(f'{test_dir}/test_data_integrity_issues.xlsx', index=False)
    
    print(f'\nâœ… æµ‹è¯•æ•°æ®é›†åˆ›å»ºå®Œæˆï¼')
    print(f'ğŸ“ ä¿å­˜ä½ç½®: {test_dir}/')
    
    # éªŒè¯åˆ›å»ºçš„æ–‡ä»¶
    for filename in ['test_data_minimal.xlsx', 'test_data_standard.xlsx', 
                     'test_data_investment.xlsx', 'test_data_complex.xlsx', 
                     'test_data_integrity_issues.xlsx']:
        filepath = f'{test_dir}/{filename}'
        if os.path.exists(filepath):
            size = os.path.getsize(filepath)
            print(f'  âœ… {filename} - {size:,} bytes')
        else:
            print(f'  âŒ {filename} - åˆ›å»ºå¤±è´¥')

def create_mock_investment_data(base_df):
    """åˆ›å»ºæ¨¡æ‹Ÿçš„æŠ•èµ„äº§å“æ•°æ®"""
    investment_df = base_df.head(500).copy()
    
    # æ·»åŠ ä¸€äº›æŠ•èµ„äº§å“ç›¸å…³çš„èµ„é‡‘å±æ€§
    investment_products = [
        'ç†è´¢-SL100613100620',
        'æŠ•èµ„-åŸºé‡‘äº§å“A',
        'ä¿é™©-äººå¯¿ä¿é™©001',
        'å…³è”é“¶è¡Œå¡-å·¥å•†é“¶è¡Œ',
        'èµ„é‡‘æ± -å›ºå®šæ”¶ç›Š001'
    ]
    
    # éšæœºæ›¿æ¢ä¸€äº›èµ„é‡‘å±æ€§
    if 'èµ„é‡‘å±æ€§' in investment_df.columns:
        n_replacements = min(100, len(investment_df))
        replacement_indices = random.sample(range(len(investment_df)), n_replacements)
        for idx in replacement_indices:
            investment_df.at[idx, 'èµ„é‡‘å±æ€§'] = random.choice(investment_products)
    
    return investment_df

def create_integrity_issues_data(base_df):
    """åˆ›å»ºåŒ…å«å®Œæ•´æ€§é—®é¢˜çš„æµ‹è¯•æ•°æ®"""
    issues_df = base_df.head(500).copy()
    
    # å¼•å…¥ä¸€äº›å®Œæ•´æ€§é—®é¢˜
    if 'ä½™é¢' in issues_df.columns:
        # 1. ä½™é¢è®¡ç®—é”™è¯¯
        for i in range(10):
            if i < len(issues_df):
                issues_df.at[i, 'ä½™é¢'] += random.uniform(-1000, 1000)
        
        # 2. åˆ›å»ºä¸€äº›ä½™é¢è·³è·ƒ
        for i in range(50, 60):
            if i < len(issues_df):
                issues_df.at[i, 'ä½™é¢'] = issues_df.at[i-1, 'ä½™é¢'] + random.uniform(10000, 50000)
    
    # 3. æ—¶é—´é¡ºåºé—®é¢˜
    if 'äº¤æ˜“æ—¥æœŸ' in issues_df.columns:
        # æ‰“ä¹±éƒ¨åˆ†æ—¥æœŸé¡ºåº
        for i in range(20, 40):
            if i < len(issues_df) and i > 0:
                # äº¤æ¢ç›¸é‚»ä¸¤è¡Œçš„æ—¥æœŸ
                temp_date = issues_df.at[i, 'äº¤æ˜“æ—¥æœŸ']
                issues_df.at[i, 'äº¤æ˜“æ—¥æœŸ'] = issues_df.at[i-1, 'äº¤æ˜“æ—¥æœŸ']
                issues_df.at[i-1, 'äº¤æ˜“æ—¥æœŸ'] = temp_date
    
    return issues_df

if __name__ == '__main__':
    print('ğŸš€ å¼€å§‹åˆ†ææ•°æ®æ ¼å¼å¹¶åˆ›å»ºæµ‹è¯•æ•°æ®é›†...')
    
    # åˆ†æç°æœ‰æ•°æ®
    original_df = analyze_existing_data()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    create_test_datasets(original_df)
    
    print('\nğŸ‰ ä»»åŠ¡å®Œæˆï¼')
